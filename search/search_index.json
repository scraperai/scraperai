{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ScraperAI \u26a1 Scraping has never been easier \u26a1 ScraperAI is an open-source, AI-powered tool designed to simplify web scraping for users of all skill levels. By leveraging Large Language Models, such as ChatGPT, ScraperAI extracts data from web pages and generates reusable and shareable scraping configs. Forget about manually extracting selectors from HTML pages using Developer Consoles. ScraperAI handles this process for you. Features Serializable & reusable Scraper Configs Automatic data detection Automatic XPATHs detection Automatic pagination & page type detection HTML minification ChatGPT support Custom LLMs support Selenium support Custom crawlers support How does ScraperAI work? ScraperAI employs AI models to analyze web pages and generate scraping tasks. These tasks are then utilized to collect data from websites in structured formats such as JSON, CSV, and others. Who should use ScraperAI? ScraperAI is designed for scientists, analysts, entrepreneurs, and SEO specialists seeking to effortlessly scrape data from the web in a no-code manner. It is particularly useful for those unfamiliar with various scraping techniques or looking to save time and effort when collecting the data. How to get started with ScraperAI? To get started with ScraperAI, you first need to install it. You can do this by running the following command: pip install scraperai Once you have installed ScraperAI, you can start using a CLI application by calling scraperai --url https://www.ycombinator.com/companies command in your console. Demo We put some examples of scraper usage here . Support If you have any questions or need help, please open a new issue.","title":"Introduction"},{"location":"#features","text":"Serializable & reusable Scraper Configs Automatic data detection Automatic XPATHs detection Automatic pagination & page type detection HTML minification ChatGPT support Custom LLMs support Selenium support Custom crawlers support","title":"Features"},{"location":"#how-does-scraperai-work","text":"ScraperAI employs AI models to analyze web pages and generate scraping tasks. These tasks are then utilized to collect data from websites in structured formats such as JSON, CSV, and others.","title":"How does ScraperAI work?"},{"location":"#who-should-use-scraperai","text":"ScraperAI is designed for scientists, analysts, entrepreneurs, and SEO specialists seeking to effortlessly scrape data from the web in a no-code manner. It is particularly useful for those unfamiliar with various scraping techniques or looking to save time and effort when collecting the data.","title":"Who should use ScraperAI?"},{"location":"#how-to-get-started-with-scraperai","text":"To get started with ScraperAI, you first need to install it. You can do this by running the following command: pip install scraperai Once you have installed ScraperAI, you can start using a CLI application by calling scraperai --url https://www.ycombinator.com/companies command in your console.","title":"How to get started with ScraperAI?"},{"location":"#demo","text":"We put some examples of scraper usage here .","title":"Demo"},{"location":"#support","text":"If you have any questions or need help, please open a new issue.","title":"Support"},{"location":"cli/","text":"CLI Application ScraperAI has a built-in CLI application. Simply run: scraperai --url https://www.ycombinator.com/companies or simply scraperai Follow the interactive process as ScraperAI attempts to auto-detect page types, pagination, catalog cards and data fields, allowing for manual correction of its detections. The CLI currently supports only the OpenAI chat model, requiring an openai_api_key . It can be provided via an environment variable, a .env file, or directly to the script. Use scraperai --help for assistance.","title":"CLI Application"},{"location":"cli/#cli-application","text":"ScraperAI has a built-in CLI application. Simply run: scraperai --url https://www.ycombinator.com/companies or simply scraperai Follow the interactive process as ScraperAI attempts to auto-detect page types, pagination, catalog cards and data fields, allowing for manual correction of its detections. The CLI currently supports only the OpenAI chat model, requiring an openai_api_key . It can be provided via an environment variable, a .env file, or directly to the script. Use scraperai --help for assistance.","title":"CLI Application"},{"location":"crawlers/","text":"Crawler Crawler primary goal is to retrieve webpage content. Therefore, various web tools can be employed, including as requests, httpx, aiohttp, Selenium, Playwright and others. However, when facing scraping tasks, additional needs may arise, such as: - Emulating human interactions (e.g., clicking, scrolling). - Capturing webpage screenshots. - Evading anti-scraping measures. In such cases, automated testing software like Selenium or Playwright is the preferred option. We opt for Selenium webdrivers as the default tool due to its convenience, user-friendly nature, and effective techniques for circumventing most website blocks. SeleniumCrawler First, initialize SeleniumCrawler: from scraperai import SeleniumCrawler crawler = SeleniumCrawler() By default, it will create a new Chrome webdriver session. You can use any other webdriver by passing it to init: from scraperai import SeleniumCrawler from selenium.webdriver.firefox.service import Service from webdriver_manager.firefox import GeckoDriverManager from selenium.webdriver import Firefox your_webdriver = Firefox(service=Service(GeckoDriverManager().install())) crawler = SeleniumCrawler(driver=your_webdriver) Selenoid ScraperAI provides additional tool to work with a pool of selenoids: from scraperai import SelenoidSettings, WebdriversManager, SeleniumCrawler # List your Selenoids selenoids = [ SelenoidSettings( url='selenoid url', max_sessions=10, capabilities={ \"browserName\": \"firefox\", \"browserVersion\": \"115.0\", \"selenoid:options\": { \"name\": \"scraperai\", \"enableVideo\": False, \"enableVNC\": True, \"sessionTimeout\": '5m' }, } ), ] # Create Webdrivers Manager that will automatically handle sessions and create new drivers manager = WebdriversManager(selenoids) # Create new driver driver = manager.create_driver() # Create crawler crawler = SeleniumCrawler(driver) RequestsCrawler You can use Requests crawler as follows: from scraperai import RequestsCrawler crawler = RequestsCrawler() However, ScraperAI partly relies on rendering functionality (pages screenshots) and interactions (scrolling for \"infinite scroll pages\" and sometimes clicking for pagination). Therefore, using requests limiting ScraperAI capabilities and may lead to poor results. Custom Crawler To implement a custom crawler, simply inherit from the following abstract class: import httpx from scraperai import BaseCrawler from scraperai.models import Pagination class YourCrawler(BaseCrawler): def __init__(self): self.response = None def get(self, url: str): self.response = httpx.get(url) @property def page_source(self) -> str: return self.response # Optional. Only for automated tests software def switch_page(self, pagination: Pagination) -> bool: raise NotImplementedError() Async Crawlers Async scraping will be introduces in the next versions of the package.","title":"Crawler"},{"location":"crawlers/#crawler","text":"Crawler primary goal is to retrieve webpage content. Therefore, various web tools can be employed, including as requests, httpx, aiohttp, Selenium, Playwright and others. However, when facing scraping tasks, additional needs may arise, such as: - Emulating human interactions (e.g., clicking, scrolling). - Capturing webpage screenshots. - Evading anti-scraping measures. In such cases, automated testing software like Selenium or Playwright is the preferred option. We opt for Selenium webdrivers as the default tool due to its convenience, user-friendly nature, and effective techniques for circumventing most website blocks.","title":"Crawler"},{"location":"crawlers/#seleniumcrawler","text":"First, initialize SeleniumCrawler: from scraperai import SeleniumCrawler crawler = SeleniumCrawler() By default, it will create a new Chrome webdriver session. You can use any other webdriver by passing it to init: from scraperai import SeleniumCrawler from selenium.webdriver.firefox.service import Service from webdriver_manager.firefox import GeckoDriverManager from selenium.webdriver import Firefox your_webdriver = Firefox(service=Service(GeckoDriverManager().install())) crawler = SeleniumCrawler(driver=your_webdriver)","title":"SeleniumCrawler"},{"location":"crawlers/#selenoid","text":"ScraperAI provides additional tool to work with a pool of selenoids: from scraperai import SelenoidSettings, WebdriversManager, SeleniumCrawler # List your Selenoids selenoids = [ SelenoidSettings( url='selenoid url', max_sessions=10, capabilities={ \"browserName\": \"firefox\", \"browserVersion\": \"115.0\", \"selenoid:options\": { \"name\": \"scraperai\", \"enableVideo\": False, \"enableVNC\": True, \"sessionTimeout\": '5m' }, } ), ] # Create Webdrivers Manager that will automatically handle sessions and create new drivers manager = WebdriversManager(selenoids) # Create new driver driver = manager.create_driver() # Create crawler crawler = SeleniumCrawler(driver)","title":"Selenoid"},{"location":"crawlers/#requestscrawler","text":"You can use Requests crawler as follows: from scraperai import RequestsCrawler crawler = RequestsCrawler() However, ScraperAI partly relies on rendering functionality (pages screenshots) and interactions (scrolling for \"infinite scroll pages\" and sometimes clicking for pagination). Therefore, using requests limiting ScraperAI capabilities and may lead to poor results.","title":"RequestsCrawler"},{"location":"crawlers/#custom-crawler","text":"To implement a custom crawler, simply inherit from the following abstract class: import httpx from scraperai import BaseCrawler from scraperai.models import Pagination class YourCrawler(BaseCrawler): def __init__(self): self.response = None def get(self, url: str): self.response = httpx.get(url) @property def page_source(self) -> str: return self.response # Optional. Only for automated tests software def switch_page(self, pagination: Pagination) -> bool: raise NotImplementedError()","title":"Custom Crawler"},{"location":"crawlers/#async-crawlers","text":"Async scraping will be introduces in the next versions of the package.","title":"Async Crawlers"},{"location":"examples/","text":"Examples YCombinator scraping Let's parse companies from YCombinator . Detection step First, initialize the the WebCrawler and ParserAI instances: from scraperai import SeleniumCrawler, ParserAI crawler = SeleniumCrawler() parser = ParserAI(openai_api_key='sk-...') Next, open the start page: start_url = 'https://www.ycombinator.com/companies/' crawler.get(start_url) We know it's a catalog page, so we'll skip page type detection. Let's detect pagination on the page: pagination = parser.detect_pagination(crawler.page_source) ## Output: Pagination using infinite scroll Next find catalog's items selector: catalog_item = parser.detect_catalog_item(page_source=crawler.page_source, website_url=start_url) ## Output: <card_xpath=\"//a[contains(@class, '_company_99gj3_339')]\" url_xpath=\"//a[contains(@class, '_company_99gj3_339')]/@href\"> For simplicity, we are not going to details pages in this example. Thus, let's find data fields in the catalog's item card: fields = parser.extract_fields(html_snippet=catalog_item.html_snippet) ## Output: 4 static fields, 0 dynamic fields Finally, Scraping step Let's wrap everything from the previous step into a ScraperConfig: from scraperai.models import ScraperConfig, WebpageType config = ScraperConfig( start_url=start_url, page_type=WebpageType.CATALOG, pagination=pagination, catalog_item=catalog_item, open_nested_pages=False, # We are not going to details pages fields=fields, max_pages=10, # Limit pages count max_rows=1000 # Limit rows count ) Now, create a Scraper instance. It will help us to collect the data. from scraperai import Scraper scraper = Scraper(config=config, crawler=crawler) Finally, iterate over the data using .scrape() generator: items = [] for item in scraper.scrape(): items.append(item) print(items) That's all! All the data can be found in items list. You may further convert it to any desired format. Jupyter notebook You can find a more detailed example in this jupyter notebook . In the notebook we present two expirements: List of YCombinator companies List of commits in the repository Other Other examples can be found here .","title":"Examples"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"examples/#ycombinator-scraping","text":"Let's parse companies from YCombinator .","title":"YCombinator scraping"},{"location":"examples/#detection-step","text":"First, initialize the the WebCrawler and ParserAI instances: from scraperai import SeleniumCrawler, ParserAI crawler = SeleniumCrawler() parser = ParserAI(openai_api_key='sk-...') Next, open the start page: start_url = 'https://www.ycombinator.com/companies/' crawler.get(start_url) We know it's a catalog page, so we'll skip page type detection. Let's detect pagination on the page: pagination = parser.detect_pagination(crawler.page_source) ## Output: Pagination using infinite scroll Next find catalog's items selector: catalog_item = parser.detect_catalog_item(page_source=crawler.page_source, website_url=start_url) ## Output: <card_xpath=\"//a[contains(@class, '_company_99gj3_339')]\" url_xpath=\"//a[contains(@class, '_company_99gj3_339')]/@href\"> For simplicity, we are not going to details pages in this example. Thus, let's find data fields in the catalog's item card: fields = parser.extract_fields(html_snippet=catalog_item.html_snippet) ## Output: 4 static fields, 0 dynamic fields Finally,","title":"Detection step"},{"location":"examples/#scraping-step","text":"Let's wrap everything from the previous step into a ScraperConfig: from scraperai.models import ScraperConfig, WebpageType config = ScraperConfig( start_url=start_url, page_type=WebpageType.CATALOG, pagination=pagination, catalog_item=catalog_item, open_nested_pages=False, # We are not going to details pages fields=fields, max_pages=10, # Limit pages count max_rows=1000 # Limit rows count ) Now, create a Scraper instance. It will help us to collect the data. from scraperai import Scraper scraper = Scraper(config=config, crawler=crawler) Finally, iterate over the data using .scrape() generator: items = [] for item in scraper.scrape(): items.append(item) print(items) That's all! All the data can be found in items list. You may further convert it to any desired format.","title":"Scraping step"},{"location":"examples/#jupyter-notebook","text":"You can find a more detailed example in this jupyter notebook . In the notebook we present two expirements: List of YCombinator companies List of commits in the repository","title":"Jupyter notebook"},{"location":"examples/#other","text":"Other examples can be found here .","title":"Other"},{"location":"getting-started/","text":"Getting Started Usage Installation Install ScraperAI easily using pip or from the source. With pip: pip install scraperai From source: git clone https://github.com/scraperai/scraperai.git pip install ./scraperai Key components Web Crawler Our WebCrawler is engineered to: Access web pages. Simulate human actions (clicking, scrolling). Capture screenshots of web pages. Selenium webdriver is the default tool due to its convenience and ease of use, incorporating techniques to avoid most website blocks. Users can implement their versions using other tools like PlayWright. The requests package is also supported, albeit with some limitations. LLMs ScraperAI heavily relies on Large Language Models (LLMs) to analyze webpage content and extract relevant information. By default, ScraperAI utilizes the latest OpenAI's ChatGPT model. HTML minifier HTML contents of the webpages are often very large comparing to the input vectors of the Large Language Models. Fortunately, HTML usually contains a lot of non-relevant information when dealing with scraping. This information includes: JS code, styling, meta information, etc. Therefore, ScraperAI introduces HTML minification tool that allows to decrease the input vector size and leave only relevant information. Usually this tool allows to decrease HTML size by 10-20 times. Page Type Detector Web pages are categorized into four types: Catalog : Pages with similar repeating elements, such as product lists, articles, companies or table rows. Details : Pages detailing information about a single product. Captcha : Captcha pages that hinder scraping efforts. Currently, we do not provide solutions to circumvent captchas. Other : All other page types not currently supported. ScraperAI primarily uses page screenshots and the GPT-4 Vision model for page type determination, with a fallback algorithm for cases where screenshots or Vision model access is unavailable. Users can manually set the page type if known. Pagination Detector This feature is applicable for catalog-type web pages, supporting: xpath : Xpath of pagination buttons like \"Next page\", \"More\", etc. scroll : Infinite scrolling. urls : a list of URLs. Catalog Item Detector This feature is specifically designed for catalog-type web pages. It identifies repeating elements that typically represent individual data items, such as products, articles, or companies. These elements may appear as visually distinct cards or as rows within a table, facilitating the organized display of information. Fields Extractor The Fields Extractor allows to detect relevant information on the page and then find XPATHs that allows to extract this detected information efficiently. This tool can be used to retrieve information from individual catalog item cards or from nested detailing pages. We define two types of data fields within HTML page: Static fields: Fields without explicit names, containing single or multiple values (e.g., product names or prices). Dynamic fields: Fields with both names and values, typically formatted like table entries.","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#usage","text":"","title":"Usage"},{"location":"getting-started/#installation","text":"Install ScraperAI easily using pip or from the source. With pip: pip install scraperai From source: git clone https://github.com/scraperai/scraperai.git pip install ./scraperai","title":"Installation"},{"location":"getting-started/#key-components","text":"","title":"Key components"},{"location":"getting-started/#web-crawler","text":"Our WebCrawler is engineered to: Access web pages. Simulate human actions (clicking, scrolling). Capture screenshots of web pages. Selenium webdriver is the default tool due to its convenience and ease of use, incorporating techniques to avoid most website blocks. Users can implement their versions using other tools like PlayWright. The requests package is also supported, albeit with some limitations.","title":"Web Crawler"},{"location":"getting-started/#llms","text":"ScraperAI heavily relies on Large Language Models (LLMs) to analyze webpage content and extract relevant information. By default, ScraperAI utilizes the latest OpenAI's ChatGPT model.","title":"LLMs"},{"location":"getting-started/#html-minifier","text":"HTML contents of the webpages are often very large comparing to the input vectors of the Large Language Models. Fortunately, HTML usually contains a lot of non-relevant information when dealing with scraping. This information includes: JS code, styling, meta information, etc. Therefore, ScraperAI introduces HTML minification tool that allows to decrease the input vector size and leave only relevant information. Usually this tool allows to decrease HTML size by 10-20 times.","title":"HTML minifier"},{"location":"getting-started/#page-type-detector","text":"Web pages are categorized into four types: Catalog : Pages with similar repeating elements, such as product lists, articles, companies or table rows. Details : Pages detailing information about a single product. Captcha : Captcha pages that hinder scraping efforts. Currently, we do not provide solutions to circumvent captchas. Other : All other page types not currently supported. ScraperAI primarily uses page screenshots and the GPT-4 Vision model for page type determination, with a fallback algorithm for cases where screenshots or Vision model access is unavailable. Users can manually set the page type if known.","title":"Page Type Detector"},{"location":"getting-started/#pagination-detector","text":"This feature is applicable for catalog-type web pages, supporting: xpath : Xpath of pagination buttons like \"Next page\", \"More\", etc. scroll : Infinite scrolling. urls : a list of URLs.","title":"Pagination Detector"},{"location":"getting-started/#catalog-item-detector","text":"This feature is specifically designed for catalog-type web pages. It identifies repeating elements that typically represent individual data items, such as products, articles, or companies. These elements may appear as visually distinct cards or as rows within a table, facilitating the organized display of information.","title":"Catalog Item Detector"},{"location":"getting-started/#fields-extractor","text":"The Fields Extractor allows to detect relevant information on the page and then find XPATHs that allows to extract this detected information efficiently. This tool can be used to retrieve information from individual catalog item cards or from nested detailing pages. We define two types of data fields within HTML page: Static fields: Fields without explicit names, containing single or multiple values (e.g., product names or prices). Dynamic fields: Fields with both names and values, typically formatted like table entries.","title":"Fields Extractor"},{"location":"llms/","text":"LLMs ScraperAI heavily relies on Large Language Models (LLMs) to analyze webpage content and extract relevant information. To ensure flexibility and quality, we integrate langchain as a core AI package. OpenAI By default, ScraperAI utilizes the latest OpenAI's ChatGPT model ( gpt-4-turbo-2024-04-09 ). To use another OpenAI model pass its name during initialization: from scraperai import JsonOpenAI model = JsonOpenAI(openai_api_key='sk-...', model_name='gpt-3.5-turbo') You can access total USD spent using total_cost property. Custom models ScraperAI provides flexibility to integrate any other model capable of generating JSON answers. Please note that although ScraperAI utilizes HTML minification, it still requires LLMs to be capable of processing substantial contexts. It's recommended that the model's input size be equal to or greater than approximately 16k Byte-Pair Encoded tokens. To implement a custom model, simply inherit from the following abstract class: from langchain_core.messages import BaseMessage from scraperai import BaseJsonLM class YourJsonModel(BaseJsonLM): def invoke(self, messages: list[BaseMessage]) -> dict: ... Then, pass your model to the ParserAI during initialization: from scraperai import ParserAI parser = ParserAI(json_lm_model=YourJsonModel(), ...) We are working on extending a list of models available out of the box. Vision Models Vision Models play a crucial role in ScraperAI for determining webpage types and generating descriptions of webpages. While usage of Vision Models is recommended, it's not mandatory. Experiments have demonstrated the effectiveness of the Vision approach compared to using JSON Models with HTML inputs. OpenAI By default, ScraperAI employs the latest OpenAI's ChatGPT model ( gpt-4-vision-preview ). To use another OpenAI model pass its name during initialization: from scraperai import VisionOpenAI model = VisionOpenAI(openai_api_key='sk-...', model_name='gpt-4-1106-vision-preview') You can access total USD spent using total_cost property. Custom models ScraperAI provides flexibility to integrate any other model by extending the abstract class as follows: from langchain_core.messages import BaseMessage from scraperai import BaseVision class YourVisionModel(BaseVision): def invoke(self, messages: list[BaseMessage]) -> str: ... Note: ScraperAI passes the image as a base64 encoded string to the image_url field of HumanMessage .","title":"LLMs"},{"location":"llms/#llms","text":"ScraperAI heavily relies on Large Language Models (LLMs) to analyze webpage content and extract relevant information. To ensure flexibility and quality, we integrate langchain as a core AI package.","title":"LLMs"},{"location":"llms/#openai","text":"By default, ScraperAI utilizes the latest OpenAI's ChatGPT model ( gpt-4-turbo-2024-04-09 ). To use another OpenAI model pass its name during initialization: from scraperai import JsonOpenAI model = JsonOpenAI(openai_api_key='sk-...', model_name='gpt-3.5-turbo') You can access total USD spent using total_cost property.","title":"OpenAI"},{"location":"llms/#custom-models","text":"ScraperAI provides flexibility to integrate any other model capable of generating JSON answers. Please note that although ScraperAI utilizes HTML minification, it still requires LLMs to be capable of processing substantial contexts. It's recommended that the model's input size be equal to or greater than approximately 16k Byte-Pair Encoded tokens. To implement a custom model, simply inherit from the following abstract class: from langchain_core.messages import BaseMessage from scraperai import BaseJsonLM class YourJsonModel(BaseJsonLM): def invoke(self, messages: list[BaseMessage]) -> dict: ... Then, pass your model to the ParserAI during initialization: from scraperai import ParserAI parser = ParserAI(json_lm_model=YourJsonModel(), ...) We are working on extending a list of models available out of the box.","title":"Custom models"},{"location":"llms/#vision-models","text":"Vision Models play a crucial role in ScraperAI for determining webpage types and generating descriptions of webpages. While usage of Vision Models is recommended, it's not mandatory. Experiments have demonstrated the effectiveness of the Vision approach compared to using JSON Models with HTML inputs.","title":"Vision Models"},{"location":"llms/#openai_1","text":"By default, ScraperAI employs the latest OpenAI's ChatGPT model ( gpt-4-vision-preview ). To use another OpenAI model pass its name during initialization: from scraperai import VisionOpenAI model = VisionOpenAI(openai_api_key='sk-...', model_name='gpt-4-1106-vision-preview') You can access total USD spent using total_cost property.","title":"OpenAI"},{"location":"llms/#custom-models_1","text":"ScraperAI provides flexibility to integrate any other model by extending the abstract class as follows: from langchain_core.messages import BaseMessage from scraperai import BaseVision class YourVisionModel(BaseVision): def invoke(self, messages: list[BaseMessage]) -> str: ... Note: ScraperAI passes the image as a base64 encoded string to the image_url field of HumanMessage .","title":"Custom models"},{"location":"parser/","text":"ParserAI ParserAI is a key component that helps to analyze the webpage, detect type, pagination, catalog item and data-fields. Start by initializing ParserAI instance: from scraperai import ParserAI # Using OpenAI parser = ParserAI(openai_api_key='sk-...') # Using Custom Models parser = ParserAI( json_lm_model=YourJsonLM(), vision_model=YourVisionLM(), # Optional ) Detecting page type Web pages are categorized into four types: Catalog : Pages with similar repeating elements, such as product lists, articles, companies or table rows. Details : Pages detailing information about a single product. Captcha : Captcha pages that hinder scraping efforts. Currently, we do not provide solutions to circumvent captchas. Other : All other page types not currently supported. ScraperAI primarily uses page screenshots and the GPT-4 Vision model for page type determination, with a fallback algorithm for cases where screenshots or Vision model access is unavailable. from scraperai import ParserAI, SeleniumCrawler # Open the page start_url = 'https://www.ycombinator.com/companies' crawler = SeleniumCrawler() crawler.get(start_url) parser = ParserAI(openai_api_key='sk-...') page_type = parser.detect_page_type( page_source=crawler.page_source, screenshot=crawler.get_screenshot_as_base64() # Optional ) Detecting pagination This feature is applicable for catalog-type web pages, supporting: xpath : Xpath of pagination buttons like \"Next page\", \"More\", etc. scroll : Infinite scrolling. urls : a list of URLs. from scraperai import ParserAI, SeleniumCrawler # Open the page start_url = 'https://www.ycombinator.com/companies' crawler = SeleniumCrawler() crawler.get(start_url) parser = ParserAI(openai_api_key='sk-...') pagination = parser.detect_pagination(crawler.page_source) Detecting catalog item This feature is specifically designed for catalog-type web pages. It identifies repeating elements that typically represent individual data items, such as products, articles, or companies. These elements may appear as visually distinct cards or as rows within a table, facilitating the organized display of information. from scraperai import ParserAI, SeleniumCrawler # Open the page start_url = 'https://www.ycombinator.com/companies' crawler = SeleniumCrawler() crawler.get(start_url) parser = ParserAI(openai_api_key='sk-...') catalog_item = parser.detect_catalog_item(crawler.page_source, start_url) print(catalog_item.url_xpath) print(catalog_item.card_xpath) Detecting data fields The Fields Extractor allows to detect relevant information on the page and then find XPATHs that allows to extract this detected information efficiently. This tool can be used to retrieve information from individual catalog item cards or from nested detailing pages. We define two types of data fields within HTML page: Static fields: Fields without explicit names, containing single or multiple values (e.g., product names or prices). Dynamic fields: Fields with both names and values, typically formatted like table entries. fields = parser.extract_fields(catalog_item.html_snippet) print(fields.static_fields) print(fields.dynamic_fields) Combine everything together After you have finished the detection process combine all data together in a ScraperConfig to pass it to a Scraper: from scraperai.models import ScraperConfig config = ScraperConfig( start_url=start_url, page_type=page_type, pagination=pagination, catalog_item=catalog_item, open_nested_pages=True, fields=fields, max_pages=10, max_rows=1000 )","title":"ParserAI"},{"location":"parser/#parserai","text":"ParserAI is a key component that helps to analyze the webpage, detect type, pagination, catalog item and data-fields. Start by initializing ParserAI instance: from scraperai import ParserAI # Using OpenAI parser = ParserAI(openai_api_key='sk-...') # Using Custom Models parser = ParserAI( json_lm_model=YourJsonLM(), vision_model=YourVisionLM(), # Optional )","title":"ParserAI"},{"location":"parser/#detecting-page-type","text":"Web pages are categorized into four types: Catalog : Pages with similar repeating elements, such as product lists, articles, companies or table rows. Details : Pages detailing information about a single product. Captcha : Captcha pages that hinder scraping efforts. Currently, we do not provide solutions to circumvent captchas. Other : All other page types not currently supported. ScraperAI primarily uses page screenshots and the GPT-4 Vision model for page type determination, with a fallback algorithm for cases where screenshots or Vision model access is unavailable. from scraperai import ParserAI, SeleniumCrawler # Open the page start_url = 'https://www.ycombinator.com/companies' crawler = SeleniumCrawler() crawler.get(start_url) parser = ParserAI(openai_api_key='sk-...') page_type = parser.detect_page_type( page_source=crawler.page_source, screenshot=crawler.get_screenshot_as_base64() # Optional )","title":"Detecting page type"},{"location":"parser/#detecting-pagination","text":"This feature is applicable for catalog-type web pages, supporting: xpath : Xpath of pagination buttons like \"Next page\", \"More\", etc. scroll : Infinite scrolling. urls : a list of URLs. from scraperai import ParserAI, SeleniumCrawler # Open the page start_url = 'https://www.ycombinator.com/companies' crawler = SeleniumCrawler() crawler.get(start_url) parser = ParserAI(openai_api_key='sk-...') pagination = parser.detect_pagination(crawler.page_source)","title":"Detecting pagination"},{"location":"parser/#detecting-catalog-item","text":"This feature is specifically designed for catalog-type web pages. It identifies repeating elements that typically represent individual data items, such as products, articles, or companies. These elements may appear as visually distinct cards or as rows within a table, facilitating the organized display of information. from scraperai import ParserAI, SeleniumCrawler # Open the page start_url = 'https://www.ycombinator.com/companies' crawler = SeleniumCrawler() crawler.get(start_url) parser = ParserAI(openai_api_key='sk-...') catalog_item = parser.detect_catalog_item(crawler.page_source, start_url) print(catalog_item.url_xpath) print(catalog_item.card_xpath)","title":"Detecting catalog item"},{"location":"parser/#detecting-data-fields","text":"The Fields Extractor allows to detect relevant information on the page and then find XPATHs that allows to extract this detected information efficiently. This tool can be used to retrieve information from individual catalog item cards or from nested detailing pages. We define two types of data fields within HTML page: Static fields: Fields without explicit names, containing single or multiple values (e.g., product names or prices). Dynamic fields: Fields with both names and values, typically formatted like table entries. fields = parser.extract_fields(catalog_item.html_snippet) print(fields.static_fields) print(fields.dynamic_fields)","title":"Detecting data fields"},{"location":"parser/#combine-everything-together","text":"After you have finished the detection process combine all data together in a ScraperConfig to pass it to a Scraper: from scraperai.models import ScraperConfig config = ScraperConfig( start_url=start_url, page_type=page_type, pagination=pagination, catalog_item=catalog_item, open_nested_pages=True, fields=fields, max_pages=10, max_rows=1000 )","title":"Combine everything together"},{"location":"scraper/","text":"Scraper Use Scraper class to collect data from webpages. Start by initializing Scraper instance: from scraperai import Scraper, SeleniumCrawler from scraperai.models import ScraperConfig config = ScraperConfig(...) # Assume you have already have a ScraperConfig crawler = SeleniumCrawler() scraper = Scraper(config=config, crawler=crawler) Now you are ready to iterate over the data using .scrape() generator: items = [] for item in scraper.scrape(): items.append(item) print(items) Afterwards transform the data in any desired format: import pandas as pd df = pd.DataFrame(items) df.to_csv('data.csv') df.to_excel('data.xlsx')","title":"Scraper"},{"location":"scraper/#scraper","text":"Use Scraper class to collect data from webpages. Start by initializing Scraper instance: from scraperai import Scraper, SeleniumCrawler from scraperai.models import ScraperConfig config = ScraperConfig(...) # Assume you have already have a ScraperConfig crawler = SeleniumCrawler() scraper = Scraper(config=config, crawler=crawler) Now you are ready to iterate over the data using .scrape() generator: items = [] for item in scraper.scrape(): items.append(item) print(items) Afterwards transform the data in any desired format: import pandas as pd df = pd.DataFrame(items) df.to_csv('data.csv') df.to_excel('data.xlsx')","title":"Scraper"}]}